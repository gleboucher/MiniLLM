# MiniLLM
#### Presentation
This repo is a basic implementation of a small llm with a transformer architecture, based on the specifications written in the [Attention is all you need ](https://arxiv.org/abs/1706.03762) paper and the help of Adndrej Karpathy's repositories. If you want to test the notebook, you only need Python, torch and the transformers package from HuggingFace. The dataset used to train is a Shakespeare dataset publicly available.
 
#### Results
The purpose of the project was not to obtain a llm ready to be used, but rather to check my ability to implement a model, and then to change a few hyperparameters or other details to better understand the whole architecture.
The final results obtain after a few hours of training were good enough to generate sentences that don't necesarily make a lot of sense but seem correct, with a loss around 3. 

#### Next
I am very motivated to explore a lot more about LLMs, that is why I would enjoy a lot to work in a Deep Learninng Laboratory specialized in Large Language Models. 

